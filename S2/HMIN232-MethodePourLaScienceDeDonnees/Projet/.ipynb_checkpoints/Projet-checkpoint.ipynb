{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39218 entries, 0 to 39217\n",
      "Data columns (total 23 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Unnamed: 0                                39218 non-null  int64  \n",
      " 1   claimReview_author                        0 non-null      float64\n",
      " 2   claimReview_author_name                   39218 non-null  object \n",
      " 3   claimReview_author_url                    0 non-null      float64\n",
      " 4   claimReview_claimReviewed                 39216 non-null  object \n",
      " 5   claimReview_datePublished                 37913 non-null  object \n",
      " 6   claimReview_source                        39218 non-null  object \n",
      " 7   claimReview_url                           39218 non-null  object \n",
      " 8   creativeWork_author_name                  17481 non-null  object \n",
      " 9   creativeWork_author_sameAs                1310 non-null   object \n",
      " 10  creativeWork_datePublished                17547 non-null  object \n",
      " 11  extra_body                                38866 non-null  object \n",
      " 12  extra_entities_author                     39218 non-null  object \n",
      " 13  extra_entities_body                       39218 non-null  object \n",
      " 14  extra_entities_claimReview_claimReviewed  39218 non-null  object \n",
      " 15  extra_entities_keywords                   39218 non-null  object \n",
      " 16  extra_refered_links                       35393 non-null  object \n",
      " 17  extra_tags                                30239 non-null  object \n",
      " 18  extra_title                               38869 non-null  object \n",
      " 19  rating_alternateName                      38964 non-null  object \n",
      " 20  rating_bestRating                         3432 non-null   float64\n",
      " 21  rating_ratingValue                        3432 non-null   float64\n",
      " 22  rating_worstRating                        3164 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(17)\n",
      "memory usage: 6.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Groupe H :\n",
    "# Nom - numéro d'étudiant\n",
    "# \n",
    "# NGUYEN Huu Khang - 21506865\n",
    "# \n",
    "# NGUYEN Hoai Nam -\n",
    "# \n",
    "# NGUYEN Tran Tuan Nam -\n",
    "#\n",
    "# TRAN Thi Tra My - 21511002\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load file\n",
    "data = pd.read_csv('Dataset/claim_extraction_18_10_2019_annotated.csv', sep=',')\n",
    "\n",
    "data2 = data.copy()\n",
    "display(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Oct. 3, 2019, town hall event in New York City hosted by Rep. Alexandria Ocasio-Cortez, D-N.Y., was going smoothly until a woman in a black jacket stood up and started rambling her fears about apocalyptic climate change. Then she took her jacket off, revealing a T-shirt that read, “Save the planet. Eat the children.”T-shirtThe woman said she was glad Ocasio-Cortez supported the Green New Deal but stated it was not enough. “Your new campaign slogan has to be this,” she stated, taking her jacket off and revealing the shirt. “We got to start eating babies. We don’t have enough time. … Even if you would bomb Russia, we still have too many people, too much pollution!”Words and images from the event shared online prompted reactions from many, including U.S. President Donald Trump and his eldest son, Donald Jr.includingSeems like a normal AOC supporter to me. https://t.co/NWVMRD0wsl— Donald Trump Jr. (@DonaldJTrumpJr) October 4, 2019Seems like a normal AOC supporter to me. https://t.co/NWVMRD0wslhttps://t.co/NWVMRD0wsl— Donald Trump Jr. (@DonaldJTrumpJr) October 4, 2019October 4, 2019During the town hall, Ocasio-Cortez responded to the woman by talking about the need for positive solutions to climate change and concluding, “We are never beyond hope” before quickly moving on to another question.Although some on the political right criticized her response, Ocasio-Cortez followed up on Twitter by explaining that she had been operating under the assumption that the woman was mentally ill and in crisis.somerightThis person may have been suffering from a mental condition and it’s not okay that the right-wing is mocking her and potentially make her condition or crisis worse. Be a decent human being and knock it off.— Alexandria Ocasio-Cortez (@AOC) October 4, 2019This person may have been suffering from a mental condition and it’s not okay that the right-wing is mocking her and potentially make her condition or crisis worse. Be a decent human being and knock it off.— Alexandria Ocasio-Cortez (@AOC) October 4, 2019October 4, 2019It turned out the woman in question was neither an Ocasio-Cortez supporter nor a person having a mental-health emergency but apparently acting at the behest of a group called Lyndon LaRouche PAC. The stunt was meant to troll Ocasio-Cortez.groupHistorian Matthew Sweet described the group as a “bizarre political cult” in an interview with The Washington Post about the incident. Sweet, who has studied the group’s history, told the Post that the woman’s stunt fell in line with their standard operating procedures. “The tactic,” Sweet told the Post, “is you go to a political meeting and you create a disturbance that disrupts the meeting, and more importantly, that creates a kind of chaos.”describedThe group, dubbed LaRouchians, has been around for decades, and was founded by conspiracy crank Lyndon LaRouche, who died in February 2019 at the age of 96 after building a “worldwide following based on conspiracy theories, economic doom, anti-Semitism, homophobia and racism.”buildingLaRouche’s conspiracy theories involve proffering claims that the Queen of England controls the international drug trade and that Adolf Hitler, whose forces bombed England in World War II, was a British agent.controlsagent\n"
     ]
    }
   ],
   "source": [
    "print(data['extra_body'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Admin123\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#initialize stop words additions \n",
    "GoWords = []\n",
    "global OurStopWords\n",
    "OurStopWords = []\n",
    "\n",
    "for word in stopwords.words('english'):\n",
    "    if GoWords.count(word) == 0:\n",
    "        OurStopWords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pré-traitement choisis : \n",
    "\n",
    "#On supprime les colonnes qui contiennent beaucoup des valeurs null\n",
    "data3 = data.copy()\n",
    "print(data3.shape)\n",
    "data3 = data3.drop('claimReview_author', 1)\n",
    "data3 = data3.drop('claimReview_author_name', 1)\n",
    "data3 = data3.drop('creativeWork_author_sameAs', 1)\n",
    "data3 = data3.drop('claimReview_author_url', 1)\n",
    "data3 = data3.drop('claimReview_source', 1)\n",
    "data3 = data3.drop('extra_entities_author', 1)\n",
    "data3 = data3.drop('extra_entities_body', 1)\n",
    "data3 = data3.drop('extra_entities_claimReview_claimReviewed', 1)\n",
    "data3 = data3.drop('extra_entities_keywords', 1)\n",
    "print(data3.shape)\n",
    "print(data3.info())\n",
    "\n",
    "######Tokenisation\n",
    "#Remove urls\n",
    "def remove_URL(text):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "#tokenize a text\n",
    "def tokennize (text) :\n",
    "    # Tokenizing\n",
    "    tokenizedText = word_tokenize(text)\n",
    "    # Put all words in lowercase\n",
    "    tokenizedText = [word.lower() for word in tokenizedText]\n",
    "    # Delete ponctuations\n",
    "    tokenizedText = [word for word in tokenizedText if word.isalpha()]\n",
    "    # Converting numbers\n",
    "    inflectEngine = inflect.engine()\n",
    "    newWords = []\n",
    "    for word in tokenizedText:\n",
    "        if word.isdigit():\n",
    "            newWords.append(inflectEngine.number_to_words(word))\n",
    "        else:\n",
    "            newWords.append(word)\n",
    "    tokenizedText = newWords\n",
    "    return tokenizedText\n",
    "\n",
    "\n",
    "#Remove non ascii charactère\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#Remove contractions\n",
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "######Stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def remove_stopwords(tokenizedText) :\n",
    "    #remove stopword\n",
    "    tokenizedText = [word for word in tokenizedText if not word in OurStopWords]\n",
    "    return tokenizedText\n",
    "\n",
    "######Lemmetisation\n",
    "\n",
    "\n",
    "######N-gram\n",
    "\n",
    "\n",
    "######POS tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6a7f30fa7a36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                NaN\n",
       "1                                NaN\n",
       "2                                NaN\n",
       "3                                NaN\n",
       "4                                NaN\n",
       "                    ...             \n",
       "39213            The Voyager Channel\n",
       "39214                   Science info\n",
       "39215                   YourNewsWire\n",
       "39216          Publications Facebook\n",
       "39217    Multiple social media posts\n",
       "Name: creativeWork_author_name, Length: 39218, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data['creativeWork_author_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claimReview_author\n",
      "claimReview_author_url\n",
      "claimReview_datePublished\n",
      "creativeWork_author_name\n",
      "creativeWork_author_sameAs\n",
      "creativeWork_datePublished\n",
      "extra_refered_links\n",
      "extra_tags\n",
      "rating_alternateName\n",
      "rating_bestRating\n",
      "rating_ratingValue\n",
      "rating_worstRating\n"
     ]
    }
   ],
   "source": [
    "#Print null column\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sickit learn met régulièrement à jour des versions et indique des futurs warnings\n",
    "# Ces deux lignes permettent de ne pas les afficher\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
