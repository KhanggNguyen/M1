{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Groupe H :\n",
    "# Nom - numéro d'étudiant\n",
    "# \n",
    "# NGUYEN Hoai Nam - 21512583 \n",
    "# \n",
    "# NGUYEN Huu Khang - 21506865\n",
    "# \n",
    "# NGUYEN Tran Tuan Nam - 21914400\n",
    "#\n",
    "# TRAN Thi Tra My - 21511002\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load file\n",
    "data = pd.read_csv('Dataset/claim_extraction_18_10_2019_annotated.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import outil sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sklearn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Sickit learn met régulièrement à jour des versions et indique des futurs warnings\n",
    "# Ces deux lignes permettent de ne pas les afficher\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "\n",
    "#Import outil nltk pour traitement\n",
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "#nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "#pickle\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize data3 \n",
    "data3 = data.copy()\n",
    "\n",
    "#Pré-traitement choisis : \n",
    "#On supprime les colonnes qui contiennent beaucoup des valeurs null\n",
    "#Les valeurs qui augmente le temps d'exécution mais n'augmente pas une précision considérable\n",
    "data3 = data3.drop('claimReview_author', 1)#manque beaucoup de valeur\n",
    "data3 = data3.drop('claimReview_author_name', 1)# celui ci baisse la precision\n",
    "data3 = data3.drop('creativeWork_author_sameAs', 1)#manque beaucoup de valeur\n",
    "data3 = data3.drop('claimReview_author_url', 1)\n",
    "data3 = data3.drop('claimReview_source', 1)#les données qui répetè beaucoup, celui ci baisse la précision\n",
    "data3 = data3.drop('extra_entities_author', 1)#rend le classification très lourd\n",
    "data3 = data3.drop('extra_entities_body', 1)#rend le classification très lourd\n",
    "data3 = data3.drop('extra_entities_claimReview_claimReviewed', 1)#rend le classification très lourd\n",
    "data3 = data3.drop('extra_entities_keywords', 1)#rend le classification très lourd\n",
    "data3 = data3.drop('extra_refered_links',1) #ne servent pas pour l'analyse et baisse la précision\n",
    "\n",
    "######Stop words\n",
    "#initialize stop words type\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#pos-tagging\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "######Pre traiter le données du texte\n",
    "def pretraiter(text):  \n",
    "    if(isinstance(text, str) and text ) :\n",
    "        #Remove contractions\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        #remove URL\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        \n",
    "        #remove photo url\n",
    "        text = re.sub(r\"pic.\\S+\", \"\", text)\n",
    "\n",
    "        # Tokenizing\n",
    "        tokenizedText = word_tokenize(text)\n",
    "        \n",
    "        #Remove non-ASCII characters from list of tokenized words\n",
    "        new_words = []\n",
    "        for word in tokenizedText:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        tokenizeText = new_words\n",
    "\n",
    "        # Put all words in lowercase\n",
    "        tokenizedText = [word.lower() for word in tokenizedText]\n",
    "\n",
    "        # Delete ponctuations\n",
    "        tokenizedText = [word for word in tokenizedText if word.isalpha()]\n",
    "\n",
    "        #remove stop_word\n",
    "        tokenizedText = [word for word in tokenizedText if not word in stop_words]\n",
    "        \n",
    "        # Converting numbers\n",
    "        p = inflect.engine()\n",
    "        newWords = []\n",
    "        for word in tokenizedText:\n",
    "            if word.isdigit():\n",
    "                newWords.append(p.number_to_words(word))\n",
    "            else:\n",
    "                newWords.append(word)\n",
    "        tokenizedText = newWords\n",
    "\n",
    "        # Lemmatization + pos-tagging\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        #tokenizedText = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokenizedText]\n",
    "        tokenizedText = [lemmatizer.lemmatize(word, 'v') for word in tokenizedText]\n",
    "        # Turning back tokens into a string\n",
    "        text = \"\".join([\" \" + i for i in tokenizedText]).strip()\n",
    "        return text\n",
    "    \n",
    "    \n",
    "##### Prétraiter alternateName\n",
    "def pretraiter_alternatename (text) :\n",
    "    if(isinstance(text, str) and text ) :\n",
    "        # Tokenizing\n",
    "        tokenizedText = word_tokenize(text)\n",
    "        \n",
    "        # Put all words in lowercase\n",
    "        tokenizedText = [word.lower() for word in tokenizedText]\n",
    "        \n",
    "        if tokenizedText[0] == 'true':\n",
    "            return 1\n",
    "        else :\n",
    "            return 0\n",
    "        \n",
    "    else :\n",
    "        return 0\n",
    "    \n",
    "def pretraiter_alternatename_bis (text) :\n",
    "    if(isinstance(text, str) and text ) :\n",
    "        # Tokenizing\n",
    "        tokenizedText = word_tokenize(text)\n",
    "        \n",
    "        # Put all words in lowercase\n",
    "        tokenizedText = [word.lower() for word in tokenizedText]\n",
    "        \n",
    "        if tokenizedText[0] == 'true' or tokenizedText[1] == 'false':\n",
    "            return 1\n",
    "        else :\n",
    "            return 0\n",
    "        \n",
    "    else :\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exécution pretraitement\n",
    "data3['extra_body'] = data['extra_body'].apply(lambda x: pretraiter(x))\n",
    "data3['claimReview_claimReviewed'] = data['claimReview_claimReviewed'].apply(lambda x: pretraiter(x))\n",
    "data3['rating_alternateName'] = data['rating_alternateName'].apply(lambda x: pretraiter_alternatename(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prétraitement avec les fonctions existants\n",
    "data3 = data3[data3['extra_body'].notna()]\n",
    "data3['claimReview_claimReviewed'] = data3['claimReview_claimReviewed'].fillna('Inconnu')\n",
    "data3['creativeWork_author_name'] = data3['creativeWork_author_name'].fillna('Inconnu')\n",
    "data3['extra_tags'] = data3['extra_tags'].fillna('Inconnu')\n",
    "data3['claimReview_datePublished'] = data3['claimReview_datePublished'].fillna('Inconnu')\n",
    "data3['creativeWork_datePublished'] = data3['creativeWork_datePublished'].fillna('Inconnu')\n",
    "moyen_bestRating = data['rating_bestRating'].mean()\n",
    "moyen_ratingValue = data['rating_ratingValue'].mean()\n",
    "moyen_worstRating = data['rating_worstRating'].mean()\n",
    "data3['rating_bestRating'] = data3['rating_bestRating'].fillna(moyen_bestRating)\n",
    "data3['rating_ratingValue'] = data3['rating_ratingValue'].fillna(moyen_ratingValue)\n",
    "data3['rating_worstRating'] = data3['rating_worstRating'].fillna(moyen_worstRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de Downsampling car nous avons beaucoup de valeur false\n",
    "data3_majority = data3[data3.rating_alternateName==0]\n",
    "data3_minority = data3[data3.rating_alternateName==1]\n",
    "data3_majority_downsampled = resample(data3_majority, replace = False, n_samples = 4594, random_state = 123)\n",
    "data3_downsampled = pd.concat([data3_majority_downsampled, data3_minority])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder() #transformer les valeurs str en valeurs numérique\n",
    "#le_data = data3.apply(le.fit_transform)\n",
    "le_data = data3_downsampled.apply(le.fit_transform)#utilisation downsampling pour les données équilibrées\n",
    "array = le_data.values\n",
    "X = array[:,1:13]\n",
    "y = array[:,9]\n",
    "\n",
    "\n",
    "validation_size=0.3 #30% du jeu de données pour le test\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=validation_size,random_state=seed,test_size=testsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation de kFold pour le classifieurs avec les données par défauts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time pour RFC   2.661090612411499\n",
      "RFC: 1.000000 (0.000000)\n",
      "Time pour LR   1.4712328910827637\n",
      "LR: 1.000000 (0.000000)\n",
      "Time pour KNN   0.15308809280395508\n",
      "KNN: 0.572196 (0.015593)\n",
      "Time pour DTC   0.03128218650817871\n",
      "DTC: 1.000000 (0.000000)\n",
      "Time pour NB   0.026218891143798828\n",
      "NB: 1.000000 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "seed = 10 \n",
    "scoring = 'accuracy'\n",
    "models = []\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "models.append(('LR', LogisticRegression(max_iter=4000)))\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors=4)))\n",
    "models.append(('DTC', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM', SVC(gamma='auto'))) #Le temps d'exécution était très important\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name,model in models:\n",
    "    kfold = KFold(n_splits=10, shuffle = True, random_state=seed)\n",
    "    start_time = time.time()\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring = scoring)\n",
    "    #pour avoir les paramètres utilisés dans le modèle enlever commentaire ligne suivante\n",
    "    #print (model.get_params())\n",
    "    print (\"Time pour\",name,\" \",time.time() - start_time)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'RFC'),\n",
       " Text(0, 0, 'LR'),\n",
       " Text(0, 0, 'KNN'),\n",
       " Text(0, 0, 'DTC'),\n",
       " Text(0, 0, 'NB')]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualiser le résultat\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Comparaison des algorithmes')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\DTC accuracy : 1.0 \n",
      "\n",
      "\n",
      " matrice de confusion \n",
      " [[3226    0]\n",
      " [   0 3206]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3226\n",
      "           1       1.00      1.00      1.00      3206\n",
      "\n",
      "    accuracy                           1.00      6432\n",
      "   macro avg       1.00      1.00      1.00      6432\n",
      "weighted avg       1.00      1.00      1.00      6432\n",
      "\n",
      "\\RFC accuracy : 1.0 \n",
      "\n",
      "\n",
      " matrice de confusion \n",
      " [[3226    0]\n",
      " [   0 3206]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3226\n",
      "           1       1.00      1.00      1.00      3206\n",
      "\n",
      "    accuracy                           1.00      6432\n",
      "   macro avg       1.00      1.00      1.00      6432\n",
      "weighted avg       1.00      1.00      1.00      6432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification DecisionTreeClassifier\n",
    "clf=DecisionTreeClassifier(random_state=40)\n",
    "clf.fit(X_train, y_train)\n",
    "result = clf.predict(X_test)\n",
    "print('\\DTC accuracy :', accuracy_score(result, y_test),'\\n')\n",
    "\n",
    "\n",
    "conf = confusion_matrix(y_test, result)\n",
    "print ('\\n matrice de confusion \\n',conf)\n",
    "print ('\\n',classification_report(y_test, result))\n",
    "\n",
    "#classification RandomForestClassifier\n",
    "clf=RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "result = clf.predict(X_test)\n",
    "print('\\RFC accuracy :', accuracy_score(result, y_test),'\\n')\n",
    "\n",
    "conf = confusion_matrix(y_test, result)\n",
    "print ('\\n matrice de confusion \\n',conf)\n",
    "print ('\\n',classification_report(y_test, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation de Gris search pour multiples classifieurs et ses parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialiser les données\n",
    "classifiers = {\n",
    "    'RandomForestClassifier' : RandomForestClassifier(),\n",
    "    'LogisticRegression' : LogisticRegression(solver='lbfgs'),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'RandomForestClassifier' :[\n",
    "        {'n_estimators': [4, 6, 9]},\n",
    "        {'max_features': ['log2', 'sqrt','auto']},\n",
    "        {'criterion': ['entropy', 'gini']},\n",
    "        {'max_depth': [2, 3, 5, 10]},\n",
    "        {'min_samples_split': [2, 3, 5]},\n",
    "        {'min_samples_leaf': [1,5,8]}],\n",
    "    \n",
    "    'LogisticRegression': [\n",
    "        {'max_iter': [4000]},\n",
    "        {'penalty': ['l2']}, \n",
    "        {'C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "    ],\n",
    "    'KNeighborsClassifier' : [\n",
    "        {'n_neighbors': list(range(1,15))}, \n",
    "        {'metric': ['minkowski','euclidean','manhattan']}\n",
    "    ],\n",
    "    'DecisionTreeClassifier' : [\n",
    "        {'max_depth': [1,2,3,4,5,6,7,8,9,10]}, \n",
    "        {'criterion': ['gini', 'entropy']}, \n",
    "        {'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10]}\n",
    "    ],\n",
    "}\n",
    "\n",
    "    \n",
    "class Result:\n",
    "    def __init__(self,name, score, parameters):\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        self.score = score\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.name, self.score, self.parameters))\n",
    "    \n",
    "results = []\n",
    "for key, value in classifiers.items():\n",
    "    gd_sr = GridSearchCV(estimator = value, \n",
    "                         param_grid = parameters[key], \n",
    "                         scoring = \"accuracy\", \n",
    "                         cv = 5,\n",
    "                         n_jobs = -1,#utilisation de plusieurs coeur pour l'exécution\n",
    "                         iid = True)\n",
    "    \n",
    "    #gd_sr.fit(X, y)#données initiales\n",
    "    gd_sr.fit(X_train, y_train)\n",
    "    \n",
    "    result = Result(key, \n",
    "                    gd_sr.best_score_,\n",
    "                    gd_sr.best_estimator_)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from best to worst: \n",
      "\n",
      "Classifier:  RandomForestClassifier  with score 1.00  avec  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=6,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False) \n",
      "\n",
      "Classifier:  LogisticRegression  with score 1.00  avec  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False) \n",
      "\n",
      "Classifier:  DecisionTreeClassifier  with score 1.00  avec  DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best') \n",
      "\n",
      "Classifier:  KNeighborsClassifier  with score 0.62  avec  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=12, p=2,\n",
      "                     weights='uniform') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = sorted(results, key = lambda result: result.score, reverse = True)\n",
    "\n",
    "print(\"Results from best to worst: \\n\")\n",
    "for result in results:\n",
    "    print (\"Classifier: \", result.name,\n",
    "    \" with score %0.2f \" %result.score, \n",
    "    \"avec \", result.parameters,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation d'une pipeline et GridSearchCV pour sauvegarder le meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création du pipeline \n",
      "\n",
      "\n",
      " accuracy: 0.7518656716417911 \n",
      "\n",
      "\n",
      "Matrice de confusion: \n",
      " [[2447  779]\n",
      " [ 817 2389]] \n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75      3226\n",
      "           1       0.75      0.75      0.75      3206\n",
      "\n",
      "    accuracy                           0.75      6432\n",
      "   macro avg       0.75      0.75      0.75      6432\n",
      "weighted avg       0.75      0.75      0.75      6432\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Création du pipeline \\n')\n",
    "clf = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "result = clf.predict(X_test)\n",
    "\n",
    "print('\\n accuracy:',accuracy_score(result, y_test),'\\n')\n",
    "\n",
    "matrix = confusion_matrix(y_test, result)\n",
    "print ('\\nMatrice de confusion: \\n', matrix, \"\\n\")\n",
    "\n",
    "print ('\\n', classification_report(y_test, result), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False) \n",
      "\n",
      "\n",
      " accuracy:\n",
      "\n",
      "1.0 \n",
      "\n",
      "\n",
      " matrice de confusion \n",
      " [[3226    0]\n",
      " [   0 3206]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3226\n",
      "           1       1.00      1.00      1.00      3206\n",
      "\n",
      "    accuracy                           1.00      6432\n",
      "   macro avg       1.00      1.00      1.00      6432\n",
      "weighted avg       1.00      1.00      1.00      6432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sauvegarder le modèle choisi\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "filename = 'pkl_modelDTC.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "clf_loaded = pickle.load(open(filename, 'rb'))\n",
    "print ('Modèle chargé',clf_loaded,'\\n')\n",
    "result = clf_loaded.predict(X_test)\n",
    "\n",
    "print('\\n accuracy:\\n')\n",
    "print (accuracy_score(result, y_test),'\\n')\n",
    "\n",
    "conf = confusion_matrix(y_test, result)\n",
    "print ('\\n matrice de confusion \\n',conf)\n",
    "print ('\\n',classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sauvegarder une csv\n",
    "data3.to_csv('clean_data_fact-checking.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
